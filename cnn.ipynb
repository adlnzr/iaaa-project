{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtOi2DDWNfcs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from config import Config, Device\n",
        "from datasets import MRIDataset, BalancedMRIDataset\n",
        "from models import CNN, CNNOneChannel\n",
        "from train import Trainer, TrainerOneChannel\n",
        "from test_file import Tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mps\n"
          ]
        }
      ],
      "source": [
        "device = Device.device\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = os.path.join(os.getcwd(), \"data\")\n",
        "labels_path = \"train.csv\"\n",
        "\n",
        "batch_size = Config.batch_size\n",
        "num_epochs = Config.num_epochs\n",
        "learning_rate = Config.learning_rate\n",
        "image_size = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# older code - not adding augmented data for the minority class \n",
        "# train_transforms = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Resize((image_size, image_size)),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(10),\n",
        "#     # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# test_transforms = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Resize((image_size, image_size)),\n",
        "#     # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new dataset code - adding augmented data for the minority class\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224))\n",
        "])\n",
        "\n",
        "augment_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "BalancedMRIDataset class is used.\n",
        "\n",
        "MRIDatset -> original unbalabed dataset. ratio: 20/80\n",
        "BalancedMRIDataset -> balanced dataset. ratio :42/58\n",
        "\n",
        "augmented data is added to the minority class \n",
        "'''\n",
        "\n",
        "\n",
        "train_dataset = BalancedMRIDataset(\n",
        "    data_path,\n",
        "    labels_path,\n",
        "    split='train',\n",
        "    transform=train_transforms,\n",
        "    augment_transform=augment_transforms,\n",
        "    max_slices=20,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset = BalancedMRIDataset(\n",
        "    data_path,\n",
        "    labels_path,\n",
        "    split='val',\n",
        "    transform=test_transforms,\n",
        "    max_slices=20\n",
        ")\n",
        "\n",
        "test_dataset = BalancedMRIDataset(\n",
        "    data_path,\n",
        "    labels_path,\n",
        "    split='test',\n",
        "    transform=test_transforms,\n",
        "    max_slices=20\n",
        ")\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dl = DataLoader(val_dataset, batch_size=32)\n",
        "test_dl = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label distribution in train_dataset:\n",
            "Label 0: 1645 samples\n",
            "Label 1: 1175 samples\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Assuming train_dataset is an instance of BalancedMRIDataset or any PyTorch Dataset\n",
        "\n",
        "# Initialize a Counter to count the labels\n",
        "label_counts = Counter()\n",
        "\n",
        "# Iterate through the dataset and update the Counter\n",
        "for _, label in train_dataset:\n",
        "    label_counts[label.item()] += 1\n",
        "\n",
        "# Print the counts\n",
        "print(\"Label distribution in train_dataset:\")\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"Label {label}: {count} samples\")\n",
        "\n",
        "\n",
        "# Label distribution in train_dataset:\n",
        "# Label 0: 1645 samples\n",
        "# Label 1: 1175 samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "as long as dataste is so inbalanced, augmented data could be added to the existing training data\n",
        "but this was not quit good idea\n",
        "- transforms is made to apply on array and image not tensors\n",
        "- transforms is applicable on 2D images therefore very confusing to apply on 3D tensor\n",
        "- more efficient performance in terms of computations and memory\n",
        "\n",
        "so best way is including this augmentation in dataset class -> BalancedMRIDataset\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (cnn1): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (cnn2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=359552, out_features=300, bias=True)\n",
              "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = CNN().to(device=device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Weights: tensor(4.0051)\n"
          ]
        }
      ],
      "source": [
        "def compute_class_weights_from_csv(csv_file_path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    labels = df['prediction'].values\n",
        "\n",
        "    # Convert labels to integers if they are not already\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    # Compute class weights\n",
        "    unique_labels = np.unique(labels)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced', classes=unique_labels, y=labels)\n",
        "\n",
        "    # Convert to torch tensor\n",
        "    return torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "\n",
        "# Path to your CSV file\n",
        "class_weights = compute_class_weights_from_csv(labels_path)\n",
        "\n",
        "# For binary classification, use the appropriate class weight\n",
        "# Assuming binary classification with class labels 0 and 1\n",
        "class_weights = class_weights[1]  # Adjust if necessary\n",
        "print(\"Class Weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The focal loss for fighting against class-imbalance\n",
        "    \"\"\"\n",
        "    def __init__(self, class_weights, device, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1e-8  # prevent training from Nan-loss error\n",
        "        self.device = device\n",
        "        \n",
        "        # Ensure class_weights is a tensor and moved to the correct device\n",
        "        self.class_weights = class_weights.clone().detach().to(self.device) if class_weights is not None else None\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        logits & target should be tensors with shape [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        probs = torch.sigmoid(logits)\n",
        "        one_subtract_probs = 1.0 - probs\n",
        "        # add epsilon\n",
        "        probs_new = probs + self.epsilon\n",
        "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
        "        # calculate focal loss\n",
        "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
        "        pt = torch.exp(log_pt)\n",
        "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
        "\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            focal_loss = focal_loss * self.class_weights\n",
        "        \n",
        "        return torch.mean(focal_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(device)\n",
        "# criterion = FocalLoss(class_weights=class_weights, device=device, alpha=0.25, gamma=2).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CNNOneChannel'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = model.__class__.__name__\n",
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    train_dl=train_dl,\n",
        "    val_dl=val_dl,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    device=device,\n",
        "    num_epochs=10,\n",
        "    patience=5,\n",
        "    threshold=0,\n",
        "    save_path=f\"saved_models/{model_name}.pth\"\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = CNN().to(device=device)\n",
        "model.load_state_dict(torch.load(\"saved_models/model_parameters1.pth\")) \n",
        "# why not CNN.pth ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tester = Tester(\n",
        "    model=model,\n",
        "    test_dl=test_dl,\n",
        "    test_dataset=test_dataset,\n",
        "    device=device,\n",
        "    threshold=0.5  # Set the threshold for binary classification\n",
        ")\n",
        "\n",
        "# Perform testing and print metrics\n",
        "tester.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
