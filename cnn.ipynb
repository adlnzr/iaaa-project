{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fateme/Projects/iaaa-project/iaaa-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Config, Device\n",
    "from datasets import BalancedMRIDataset\n",
    "from models import CNN\n",
    "from trainer import Trainer\n",
    "from tester import Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = Device.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "labels_path = \"train.csv\"\n",
    "\n",
    "batch_size = Config.batch_size\n",
    "num_epochs = Config.num_epochs\n",
    "learning_rate = Config.learning_rate\n",
    "mean = Config.mean # mean of the entire datasaet\n",
    "std = Config.std # std of the entire dataaset\n",
    "image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset code - adding augmented data for the minority class\n",
    "\n",
    "'''\n",
    "BalancedMRIDataset -> adding augmented data for the minority class to avoide unbalanced data\n",
    "\n",
    "train_transforms -> applied to train & majority class \n",
    "augment_transforms -> applied to train & minority class\n",
    "\n",
    "ToTensor -> re-scales the data to the range [0,1]\n",
    "\n",
    "Note -> in case of pretrained models typically: Normalize(mean=0.5, std=0.5)\n",
    "'''\n",
    "\n",
    "resclaed_mean = round(mean/255,4) # re-scale the actual mean\n",
    "rescaled_std = round(std/255, 4) # re-scale the actual std\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[resclaed_mean], std=[rescaled_std])\n",
    "])\n",
    "\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    # transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[resclaed_mean], std=[rescaled_std])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    # transforms.Lambda(lambda img: img.astype(np.float32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[resclaed_mean], std=[rescaled_std])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BalancedMRIDataset class is used.\n",
    "\n",
    "MRIDatset -> original unbalabed dataset. ratio: 20/80\n",
    "BalancedMRIDataset -> balanced dataset. ratio :42/58\n",
    "\n",
    "augmented data is added to the minority class \n",
    "\n",
    "random seceltion padding is used instead of zero padding\n",
    "'''\n",
    "\n",
    "train_dataset = BalancedMRIDataset(\n",
    "    data_path,\n",
    "    labels_path,\n",
    "    split='train',\n",
    "    transform=train_transforms,\n",
    "    augment_transform=augment_transforms,\n",
    "    augment=True,\n",
    "    max_slices=20\n",
    ")\n",
    "\n",
    "val_dataset = BalancedMRIDataset(\n",
    "    data_path,\n",
    "    labels_path,\n",
    "    split='val',\n",
    "    transform=test_transforms,\n",
    "    max_slices=20\n",
    ")\n",
    "\n",
    "test_dataset = BalancedMRIDataset(\n",
    "    data_path,\n",
    "    labels_path,\n",
    "    split='test',\n",
    "    transform=test_transforms,\n",
    "    max_slices=20\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=32)\n",
    "test_dl = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_, label_ = next(iter(train_dataset))\n",
    "data_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_, label_ = next(iter(train_dl))\n",
    "data_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNOneChannel(\n",
       "  (cnn1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=359552, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor(4.0051)\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights_from_csv(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    labels = df['prediction'].values\n",
    "\n",
    "    # Convert labels to integers if they are not already\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    # Compute class weights\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced', classes=unique_labels, y=labels)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    return torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Path to your CSV file\n",
    "class_weights = compute_class_weights_from_csv(labels_path)\n",
    "\n",
    "# For binary classification, use the appropriate class weight\n",
    "# Assuming binary classification with class labels 0 and 1\n",
    "class_weights = class_weights[1]  # Adjust if necessary\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, class_weights, device, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-8  # prevent training from Nan-loss error\n",
    "        self.device = device\n",
    "        \n",
    "        # Ensure class_weights is a tensor and moved to the correct device\n",
    "        self.class_weights = class_weights.clone().detach().to(self.device) if class_weights is not None else None\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            focal_loss = focal_loss * self.class_weights\n",
    "        \n",
    "        return torch.mean(focal_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "'''\n",
    "dataset is almost balanced so pos_weight and FocalLoss is not chosen \n",
    "'''\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights).to(device)\n",
    "# criterion = FocalLoss(class_weights=class_weights, device=device, alpha=0.25, gamma=2).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [12:35<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[8500 2460]\n",
      " [1180  380]]\n",
      "Epoch 1/2, Train Loss: 750.6379, Train Accuracy: 0.8369\n",
      "Epoch 1/2, Val Accuracy: 14.1853, Precision: 0.1338, Recall: 0.2436, AUC: 0.5096, Avg Metric: 0.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [12:45<00:00,  8.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[9260 1700]\n",
      " [1140  420]]\n",
      "Epoch 2/2, Train Loss: 575.1818, Train Accuracy: 0.8780\n",
      "Epoch 2/2, Val Accuracy: 15.4633, Precision: 0.1981, Recall: 0.2692, AUC: 0.5571, Avg Metric: 0.3415\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model : CNN\n",
    "model output: linear (no softmax)\n",
    "criterion: BCEWithLogitLoss - the criterion applies softmax to the output\n",
    "assuming balanced dataset class weights and focal loss are not considered\n",
    " \n",
    "'''\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    device=device,\n",
    "    num_epochs=2,\n",
    "    patience=5,\n",
    "    threshold=0.5,\n",
    "    save_path=f\"saved_models/{model_name}.pth\"\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device=device)\n",
    "model.load_state_dict(torch.load(\"saved_models/CNNOneChannel.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 14.7284, Precision: 0.0857, Recall: 0.1154, AUC: 0.4701, Avg Metric: 4.9765\n"
     ]
    }
   ],
   "source": [
    "tester = Tester(\n",
    "    model=model,\n",
    "    criterion = criterion,\n",
    "    test_dl=test_dl,\n",
    "    criterion=criterion,\n",
    "    test_dataset=test_dataset,\n",
    "    device=device,\n",
    "    threshold=0.5  # Set the threshold for binary classification\n",
    ")\n",
    "\n",
    "# Perform testing and print metrics\n",
    "tester.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
